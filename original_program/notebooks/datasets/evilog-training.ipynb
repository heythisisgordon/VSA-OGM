{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from vsa_ogm.dataloaders.dl_evilog import EviLogDataLoader\n",
    "from spl.mapping import OGM2D_V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = DictConfig({\n",
    "    \"mapper\": {\n",
    "        \"axis_resolution\": 0.1, # meters\n",
    "        \"decision_thresholds\": [-0.99, 0.99],\n",
    "        \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"length_scale\": 0.2,\n",
    "        \"quadrant_hierarchy\": [1],\n",
    "        \"use_query_normalization\": True,\n",
    "        \"use_query_rescaling\": False,\n",
    "        \"verbose\": True,\n",
    "        \"vsa_dimensions\": 32000,\n",
    "        \"plotting\": {\n",
    "            \"plot_xy_voxels\": False\n",
    "        }\n",
    "    },\n",
    "})\n",
    "WORLD_SIZE = [0, 9, 0, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sample(points, labels):\n",
    "    mapper = OGM2D_V4(BASE_CONFIG.mapper, WORLD_SIZE, \".\")\n",
    "\n",
    "    mapper.process_observation(points, labels)\n",
    "\n",
    "    occ_hm = mapper.xy_axis_occupied_heatmap.cpu().numpy()\n",
    "    empty_hm = mapper.xy_axis_empty_heatmap.cpu().numpy()\n",
    "\n",
    "    return occ_hm, empty_hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_heatmaps = []\n",
    "empty_heatmaps = []\n",
    "input_imgs = []\n",
    "target_imgs = []\n",
    "points_list = []\n",
    "labels_list = []\n",
    "\n",
    "loader_config = DictConfig({\n",
    "    \"data_dir\": \"/home/ssnyde9/dev/EviLOG/model/output/2024-02-12-12-51-25/Evaluation\"\n",
    "})\n",
    "\n",
    "loader = EviLogDataLoader(loader_config)\n",
    "\n",
    "counter = 0\n",
    "lidar_points, labels, input_img, target_img = loader.reset()\n",
    "lidar_points *= 0.1\n",
    "\n",
    "while counter < loader.max_steps() - 1:\n",
    "    occ_hm, empty_hm = eval_sample(lidar_points, labels)\n",
    "\n",
    "    occ_heatmaps.append(occ_hm)\n",
    "    empty_heatmaps.append(empty_hm)\n",
    "    input_imgs.append(input_img)\n",
    "    target_imgs.append(target_img)\n",
    "    points_list.append(lidar_points)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "    lidar_points, labels, input_img, target_img = loader.step()\n",
    "    lidar_points *= 0.1\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from sklearn import metrics\n",
    "from highfrost.ogm.metrics import calculate_multiple_TP_FP_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_entropy_list = []\n",
    "occ_disk = disk(2)\n",
    "empty_disk = disk(4)\n",
    "\n",
    "image_dir = os.path.join(\".\", \"images\")\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "for i in range(len(occ_heatmaps)):\n",
    "    observation_dir = os.path.join(image_dir, f\"observation_{i}\")\n",
    "    os.makedirs(observation_dir, exist_ok=True)\n",
    "\n",
    "    occ_hm = occ_heatmaps[i]\n",
    "    empty_hm = empty_heatmaps[i]\n",
    "    input_img = input_imgs[i]\n",
    "    target_img = target_imgs[i]\n",
    "\n",
    "    plt.imshow(occ_hm, cmap=\"plasma\")\n",
    "    plt.colorbar()\n",
    "    plt.clim(-1, 1)\n",
    "    plt.savefig(os.path.join(observation_dir, \"occ_heatmap.png\"), dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    plt.imshow(empty_hm, cmap=\"plasma\")\n",
    "    plt.colorbar()\n",
    "    plt.clim(-1, 1)\n",
    "    plt.savefig(os.path.join(observation_dir, \"empty_heatmap.png\"), dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    plt.imshow(input_img)\n",
    "    plt.savefig(os.path.join(observation_dir, \"input_img.png\"), dpi=500)\n",
    "\n",
    "    plt.imshow(target_img)\n",
    "    plt.savefig(os.path.join(observation_dir, \"target_img.png\"), dpi=500)\n",
    "\n",
    "    occ_entropy = entropy(np.square(occ_hm), occ_disk)\n",
    "    empty_entropy = entropy(np.square(empty_hm), empty_disk)\n",
    "    global_entropy = occ_entropy - empty_entropy\n",
    "\n",
    "    plt.imshow(occ_entropy)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(observation_dir, \"occ_entropy.png\"), dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    plt.imshow(empty_entropy)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(observation_dir, \"empty_entropy.png\"), dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    plt.imshow(global_entropy)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(observation_dir, \"global_entropy.png\"), dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    global_entropy_list.append(global_entropy)\n",
    "\n",
    "    print(f\"Saved observation {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_preds_w_threshold_test(ge: np.ndarray, threshold: float, lidar_points: np.ndarray, labels: np.ndarray):\n",
    "    points = np.copy(lidar_points)\n",
    "    labels_ = np.copy(labels)\n",
    "\n",
    "    points = points[points[:,0] >= WORLD_SIZE[0], :]\n",
    "    points = points[points[:,0] <= WORLD_SIZE[1], :]\n",
    "    points = points[points[:,1] >= WORLD_SIZE[2], :]\n",
    "    points = points[points[:,1] <= WORLD_SIZE[3], :]\n",
    "    labels_ = labels_[points[:,0] >= WORLD_SIZE[0]]\n",
    "    labels_ = labels_[points[:,0] <= WORLD_SIZE[1]]\n",
    "    labels_ = labels_[points[:,1] >= WORLD_SIZE[2]]\n",
    "    labels_ = labels_[points[:,1] <= WORLD_SIZE[3]]  \n",
    "\n",
    "    assert points.shape[0] == labels.shape[0]\n",
    "    assert points.shape[0] > 0\n",
    "\n",
    "    points[:, 0] -= WORLD_SIZE[0]\n",
    "    points[:, 1] -= WORLD_SIZE[2]\n",
    "    points /= 0.1\n",
    "    points = points.astype(np.uint8)\n",
    "\n",
    "    assert points.shape[0] == labels_.shape[0]\n",
    "    assert points.shape[0] > 0\n",
    "\n",
    "    e_values = ge[points[:, 1], points[:, 0]]\n",
    "\n",
    "    assert e_values.shape[0] == points.shape[0]\n",
    "    \n",
    "    preds = np.zeros(shape=(points.shape[0]))\n",
    "    preds[e_values > threshold] = 1\n",
    "\n",
    "    assert preds.shape[0] == points.shape[0]\n",
    "\n",
    "    return labels_, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list = []\n",
    "f1_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "\n",
    "for i in range(len(global_entropy_list)):\n",
    "    global_entropy = global_entropy_list[i]\n",
    "\n",
    "    threshold_min = np.min(global_entropy)\n",
    "    threshold_max = np.max(global_entropy)\n",
    "    threshold_step_size = 0.01\n",
    "    threshold_range = np.arange(threshold_min, threshold_max, threshold_step_size)\n",
    "\n",
    "    y_true: list[np.ndarray] = []\n",
    "    y_pred: list[np.ndarray] = []\n",
    "\n",
    "    for t in threshold_range:\n",
    "        true, pred = calculate_preds_w_threshold_test(global_entropy, t, points_list[i], labels_list[i])\n",
    "        \n",
    "        y_true.append(true)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    tpr_list, fpr_list = calculate_multiple_TP_FP_rates(y_true, y_pred)\n",
    "    auc = metrics.auc(fpr_list, tpr_list)\n",
    "\n",
    "    auc_list.append(auc)\n",
    "\n",
    "    f1_scores = []\n",
    "    for i in range(len(threshold_range)):\n",
    "        f1_scores.append(metrics.f1_score(y_true[i], y_pred[i]))\n",
    "\n",
    "    best_threshold = threshold_range[np.argmax(f1_scores)]\n",
    "\n",
    "    f1_list.append(np.max(f1_scores))\n",
    "    precision_list.append(metrics.precision_score(y_true[np.argmax(f1_scores)], y_pred[np.argmax(f1_scores)]))\n",
    "    recall_list.append(metrics.recall_score(y_true[np.argmax(f1_scores)], y_pred[np.argmax(f1_scores)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(auc_list)\n",
    "plt.title(\"AUC Scores\")\n",
    "print(f\"Mean AUC: {np.mean(auc_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f1_list)\n",
    "plt.title(\"F1 Scores\")\n",
    "print(f\"Mean F1: {np.mean(f1_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precision_list)\n",
    "plt.title(\"Precision Scores\")\n",
    "print(f\"Mean Precision: {np.mean(precision_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall_list)\n",
    "plt.title(\"Recall Scores\")\n",
    "print(f\"Mean Recall: {np.mean(recall_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "army-vsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
